{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One shot learning (UNFINISHED).\n",
    "\n",
    "Imagine we have a bunch of classes, say 10 classes, and we'd like to perform a classification task. However, we only have a good amount of data for 8 classes out of 10. For the other 2 classes, there's only a very limited number of examples (say 1 or 2 examples for instance). The idea of *one shot learning* is to train a network on the classes for which we have a lot of data and use this trained network to classify examples from the classes for which it wasn't trained on. Here, we mostly follow the approach described in *Siamese Neural Networks for One-shot Image Recognition* by Koch et al.\n",
    "\n",
    "We use a siamese architecture that we train on the MNIST data set. More specifically, we only train the netowk on digits from 0 to 7. The network will take two images and answer the following question: **do the two inputs belong to the same class?** After the training has been completed, we try to classify the digits 8 and 9 by comparing the testing examples to the very limited labeled data we have for these classes.\n",
    "\n",
    "For more details on *siamese architecture*, we refer the interested reader to the implementation of a siamese network in the notebook **siamese**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm # just for esthetics (progression bar)\n",
    "sys.path.insert(0, '../data_processing/')\n",
    "from siamese_data import MNIST # load the data and process it\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = MNIST()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning a similarity metric with a siamese network\n",
    "\n",
    "We are going to implement a siamese architecture similar to the one described in the **siamese notebook**, but with a stacked bi-directional LSTM network instead of a vanilla bi-directional LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first fix the hyperparameters for:\n",
    "- The training:\n",
    "    - Number of iterations,\n",
    "    - Learning rate,\n",
    "    - Batch size.\n",
    "- The network architectures:\n",
    "    - Number of stacked LSTMs,\n",
    "    - Number of neurons of each LSTM cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_iter = 15000 # maximum number of iterations for training\n",
    "learning_rate = 0.001\n",
    "batch_train = 128 # batch size for training\n",
    "batch_test = 512 # batch size for testing\n",
    "display = 100 # display the training loss and accuracy every `display` step\n",
    "n_test = 200 # how frequently to test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_inputs = 28 # dimension of each of the input vectors\n",
    "n_steps = 28 # sequence length\n",
    "n_hidden = [128, 64, 64] # number of neurons of each of the LSTM cell.\n",
    "n_classes = 2 # two possible classes, either `same` of `different`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    x1 = tf.placeholder(tf.float32, shape=[None, n_steps, n_inputs]) # placeholder for the first network (image 1)\n",
    "    x2 = tf.placeholder(tf.float32, shape=[None, n_steps, n_inputs]) # placeholder for the second network (image 2)\n",
    "\n",
    "    # placeholder for the label. `1` for `same` and `0` for `different`.\n",
    "    y = tf.placeholder(tf.int64, shape=[None])\n",
    "\n",
    "    # placeholder for dropout (we could use different dropout for different part of the architecture)\n",
    "    keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reshape_input(x_):\n",
    "    \"\"\"\n",
    "    Reshape the inputs to match the shape requirements of the function\n",
    "    `tf.nn.bidirectional_rnn`\n",
    "    \n",
    "    Args:\n",
    "        x_: a tensor of shape `(batch_size, n_steps, n_inputs)`\n",
    "        \n",
    "    Returns: a `list` of length `n_steps` with its elements being tensors\n",
    "    of shape `(batch_size, n_inputs)`\n",
    "    \"\"\"\n",
    "    x_ = tf.transpose(x_, [1, 0, 2]) # shape: (n_steps, batch_size, n_inputs)\n",
    "    x_ = tf.split(0, n_steps, x_) # a list of `n_steps` tensors of shape (1, batch_size, n_steps)\n",
    "    return [tf.squeeze(z, [0]) for z in x_] # remove size 1 dimension --> (batch_size, n_steps)\n",
    "\n",
    "\n",
    "x1_, x2_ = reshape_input(x1), reshape_input(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net(x_):\n",
    "    \"\"\"\n",
    "    Defines the network.\n",
    "    \n",
    "    Args:\n",
    "        x_: a tensor of shape `(batch_size, n_steps, n_inputs)` containing a batch\n",
    "            of images that will be fed to one of the two networks.\n",
    "    \n",
    "    Returns the last states from the forward and backward cell.\n",
    "    \"\"\"    \n",
    "    lstm_cells_fw = []\n",
    "    lstm_cells_bw = []\n",
    "    for hid_units in n_hidden:\n",
    "        lstm_cells_fw.append(tf.nn.rnn_cell.BasicLSTMCell(hid_units, forget_bias=1.0, state_is_tuple=True))\n",
    "        lstm_cells_bw.append(tf.nn.rnn_cell.BasicLSTMCell(hid_units, forget_bias=1.0, state_is_tuple=True))\n",
    "    stacked_lstm_fw = tf.nn.rnn_cell.MultiRNNCell(lstm_cells_fw, state_is_tuple=True)\n",
    "    stacked_lstm_bw = tf.nn.rnn_cell.MultiRNNCell(lstm_cells_bw, state_is_tuple=True)\n",
    "    \n",
    "    stacked_lstm_fw = tf.nn.rnn_cell.DropoutWrapper(stacked_lstm_fw, output_keep_prob=keep_prob)\n",
    "    stacked_lstm_bw = tf.nn.rnn_cell.DropoutWrapper(stacked_lstm_bw, output_keep_prob=keep_prob)\n",
    "    \n",
    "    \n",
    "    _, last_state_fw, last_state_bw = tf.nn.bidirectional_rnn(\n",
    "                                        stacked_lstm_fw, stacked_lstm_bw, x_,\n",
    "                                        dtype=tf.float32)\n",
    "    return last_state_fw, last_state_bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    with tf.variable_scope('siamese_network') as scope:\n",
    "        with tf.name_scope('network_1'):\n",
    "            last_state_fw1, last_state_bw1 = net(x1_)\n",
    "        with tf.name_scope('network_2'):\n",
    "            scope.reuse_variables() # tied weights (reuse the weights from `network_1` for `network_2`)\n",
    "            last_state_fw2, last_state_bw2 = net(x2_)\n",
    "\n",
    "    last_state1 = []\n",
    "    last_state2 = []\n",
    "    for i in range(len(n_hidden)):\n",
    "        for j in range(2):\n",
    "            last_state1.extend([last_state_bw1[i][j], last_state_fw1[i][j]])\n",
    "            last_state2.extend([last_state_bw2[i][j], last_state_fw2[i][j]])\n",
    "\n",
    "    last_state1 = tf.concat(1, last_state1) # We concatenate the states of the first network\n",
    "    last_state2 = tf.concat(1, last_state2) # We concatenate the states of the second network\n",
    "\n",
    "    # Weights and biases for the layer that connects the outputs from the two networks\n",
    "    weights = tf.get_variable('weigths_out', shape=[4 * np.sum(n_hidden), n_classes],\n",
    "                    initializer=tf.random_normal_initializer(stddev=1.0/float(np.sum(n_hidden))))\n",
    "    biases = tf.get_variable('biases_out', shape=[n_classes])\n",
    "\n",
    "    # difference between the states from the two networks\n",
    "    last_states_diff = tf.abs(last_state1 - last_state2) \n",
    "    logits = tf.matmul(last_states_diff, weights) + biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), y) \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_snapshot = 1000 # save the weights every `n_snapshot` step\n",
    "checkpoint_dir = '../models/one_shot_learning/'\n",
    "saver = tf.train.Saver() # to save the trained model and, later, to restore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network training begins.\n",
      "step 100, training loss: 0.37932, training accuracy: 0.781\n",
      "step 200, training loss: 0.29217, training accuracy: 0.883\n",
      "testing step 200, accuracy 0.877\n",
      "step 300, training loss: 0.15428, training accuracy: 0.922\n",
      "step 400, training loss: 0.11099, training accuracy: 0.961\n",
      "testing step 400, accuracy 0.939\n",
      "step 500, training loss: 0.06529, training accuracy: 0.977\n",
      "step 600, training loss: 0.03001, training accuracy: 0.992\n",
      "testing step 600, accuracy 0.975\n",
      "step 700, training loss: 0.10999, training accuracy: 0.969\n",
      "step 800, training loss: 0.05453, training accuracy: 0.977\n",
      "testing step 800, accuracy 0.980\n",
      "step 900, training loss: 0.09954, training accuracy: 0.977\n",
      "step 1000, training loss: 0.03332, training accuracy: 0.984\n",
      "testing step 1000, accuracy 0.975\n",
      "Snapshot saved in file: ../models/one_shot_learning/snapshot_1000.ckpt\n",
      "step 1100, training loss: 0.04202, training accuracy: 0.984\n",
      "step 1200, training loss: 0.15901, training accuracy: 0.938\n",
      "testing step 1200, accuracy 0.984\n",
      "step 1300, training loss: 0.02680, training accuracy: 0.992\n",
      "step 1400, training loss: 0.06985, training accuracy: 0.984\n",
      "testing step 1400, accuracy 0.992\n",
      "step 1500, training loss: 0.02323, training accuracy: 0.992\n",
      "step 1600, training loss: 0.03745, training accuracy: 0.977\n",
      "testing step 1600, accuracy 0.979\n",
      "step 1700, training loss: 0.02788, training accuracy: 0.984\n",
      "step 1800, training loss: 0.00871, training accuracy: 1.000\n",
      "testing step 1800, accuracy 0.990\n",
      "step 1900, training loss: 0.00353, training accuracy: 1.000\n",
      "step 2000, training loss: 0.00799, training accuracy: 1.000\n",
      "testing step 2000, accuracy 0.984\n",
      "Snapshot saved in file: ../models/one_shot_learning/snapshot_2000.ckpt\n",
      "step 2100, training loss: 0.02319, training accuracy: 0.992\n",
      "step 2200, training loss: 0.03732, training accuracy: 0.984\n",
      "testing step 2200, accuracy 0.986\n",
      "step 2300, training loss: 0.06614, training accuracy: 0.992\n",
      "step 2400, training loss: 0.06840, training accuracy: 0.969\n",
      "testing step 2400, accuracy 0.980\n",
      "step 2500, training loss: 0.00654, training accuracy: 1.000\n",
      "step 2600, training loss: 0.00983, training accuracy: 1.000\n",
      "testing step 2600, accuracy 0.992\n",
      "step 2700, training loss: 0.03236, training accuracy: 0.984\n",
      "step 2800, training loss: 0.00491, training accuracy: 1.000\n",
      "testing step 2800, accuracy 0.977\n",
      "step 2900, training loss: 0.02757, training accuracy: 0.984\n",
      "step 3000, training loss: 0.02452, training accuracy: 0.984\n",
      "testing step 3000, accuracy 0.982\n",
      "Snapshot saved in file: ../models/one_shot_learning/snapshot_3000.ckpt\n",
      "step 3100, training loss: 0.01904, training accuracy: 0.992\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c0285c6c9c3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# We feed the data to the network for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m.9\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 372\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    373\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 636\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    637\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m       \u001b[0;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 708\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    709\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    713\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    695\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    696\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# the argument `allow_soft_placement=True` indicates that if a given function is\n",
    "# not implemented for GPUs, tensorflow will automatically use its CPU counterpart.\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "    sess.run(init) # initialize all variables\n",
    "    print('Network training begins.')\n",
    "    for i in range(1, max_iter + 1):\n",
    "        # We retrieve a batch of data from the training set\n",
    "        batch_x1, batch_x2, batch_y = data.get_next_batch(batch_train, phase='train', one_shot=True)\n",
    "        # We feed the data to the network for training\n",
    "        feed_dict = {x1: batch_x1, x2: batch_x2, y: batch_y, keep_prob: .75}\n",
    "        _, loss_, accuracy_ = sess.run([optimizer, loss, accuracy], feed_dict=feed_dict)\n",
    "        \n",
    "        if i % display == 0:\n",
    "            print('step %i, training loss: %.5f, training accuracy: %.3f' % (i, loss_, accuracy_))\n",
    "        \n",
    "        # Testing the network\n",
    "        if i % n_test == 0:\n",
    "            # Retrieving data from the test set\n",
    "            batch_x1, batch_x2, batch_y = data.get_next_batch(batch_test, phase='test', one_shot=True)\n",
    "            feed_dict = {x1: batch_x1, x2: batch_x2, y: batch_y, keep_prob: 1.0}\n",
    "            accuracy_test = sess.run(accuracy, feed_dict=feed_dict)\n",
    "            print('testing step %i, accuracy %.3f' % (i, accuracy_test))\n",
    "            \n",
    "            \n",
    "        # We save a snapshot of the weights\n",
    "        if i % n_snapshot == 0:\n",
    "            save_path = saver.save(sess, os.path.join(checkpoint_dir,'snapshot_') + str(i) + '.ckpt')\n",
    "            print('Snapshot saved in file: %s' % save_path)\n",
    "            \n",
    "    print('********************************')\n",
    "    print('Training finished.')\n",
    "    \n",
    "    # testing the trained network on a large sample\n",
    "    batch_x1, batch_x2, batch_y = data.get_next_batch(10000, phase='test', one_shot=True)\n",
    "    feed_dict = {x1: batch_x1, x2: batch_x2, y: batch_y, keep_prob:1.0}\n",
    "    accuracy_test = sess.run(accuracy, feed_dict=feed_dict)\n",
    "    print('********************************')\n",
    "    print('Testing the network.')\n",
    "    print('Network accuracy %.3f' % (accuracy_test))\n",
    "    print('********************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One shot learning: using the pretrained similarity metric on new classes\n",
    "\n",
    "We now want to see how the network performs on images from unseen classes, i.e. eights and nines.\n",
    "\n",
    "Following the approach described by Koch et al., we chose 10 images ($i_0, i_1,...,i_9$), one per class. We then classify an image by comparing it pairwise with the images $i_0,...,i_9$.\n",
    "\n",
    "But first, let's chose 10 reference images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_example_per_class = []\n",
    "for digit in data.digits:\n",
    "    one_example_per_class.append(\n",
    "        getattr(data, digit + '_train')[\n",
    "            np.random.randint(len(getattr(data, digit + '_train')))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we display the 10 images we use ($i_0,...,i_9$) as a benchmark for pairwise comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,2))\n",
    "for i in range(1, 11):\n",
    "    a = fig.add_subplot(2, 5, i)\n",
    "    a.axis('off')\n",
    "    image = one_example_per_class[i - 1].reshape((28, 28)) # reshape the image from (784) to (28, 28).\n",
    "    a.imshow(image, cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve the model trained above, and we classify images of 8 and 9 by comparing them with the benchmark images $i_0,...,i_9$. We report the accuracy of the classifcation on the *seen* and *unseen* classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reshape_input(image):\n",
    "    \"\"\"\n",
    "    Reshapes an image from `(784)` to `(1, 28, 28)`.\n",
    "    \n",
    "    Args:\n",
    "        image: a `numpy array` of shape `(784)`.\n",
    "    \n",
    "    Returns  a `numpy array` of shape `(1, 28, 28)`.\n",
    "    \"\"\"\n",
    "    image = np.expand_dims(image.reshape((28,28)), axis=0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_benchmark(images):\n",
    "    \"\"\"\n",
    "    Concatenates the 10 images of the benchmark into one tensor.\n",
    "    \n",
    "    Args:\n",
    "        images: a `list` of ten `numpy array`s of shape (784).\n",
    "    \n",
    "    Returns a `numpy array` of shape `(10, 28, 28)`.   \n",
    "    \"\"\"\n",
    "    images = [reshape_input(x) for x in images]\n",
    "    return np.concatenate(images)\n",
    "        \n",
    "\n",
    "def duplicate_input(image):\n",
    "    \"\"\"\n",
    "    Duplicates the image ten times.\n",
    "    \n",
    "    Args:\n",
    "        image: a `numpy array` of shape (784).\n",
    "    \n",
    "    Returns a `numpy array` of shape (10, 28, 28).\n",
    "    \"\"\"\n",
    "    image = reshape_input(image)\n",
    "    image = [image for x in range(10)]\n",
    "    return np.concatenate(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prediction_bunch(predictions, bunch=32):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        predictions: a `numpy array` of shape `(10 * bunch, 2)`. The second\n",
    "            column contains the probability that the given inputs are similar.\n",
    "        bunch: an `integer` equal the to the batch size divided by 10.\n",
    "    \n",
    "    Returns a list of length `bunch` containing the predicted labels, i.e.\n",
    "    a list of integers between 0 and 9.\n",
    "    \"\"\"\n",
    "    predictions_ = []\n",
    "    for i in range(bunch):\n",
    "        predictions_.append(np.argmax(predictions[10 * i : 10 * (i + 1), 1]))\n",
    "    return predictions_\n",
    "\n",
    "def test_number(data_, benchmark, sess, bunch=32):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data_: a `list` of `numpy array`s containing images from a specific class\n",
    "            (e.g. only 5 or only 9). The images have a shape `(784)`.\n",
    "        benchmark: a `numpy array` of shape `(10, 28, 28)`.\n",
    "        sess: a tensorflow session.\n",
    "        bunch: an `integer` equal the to the batch size divided by 10. It represents\n",
    "            the number of different images being fed to the network at the same time.\n",
    "    \n",
    "    Returns a list of length `bunch` containing the label predictions.\n",
    "    \"\"\"\n",
    "    benchmark_ = np.concatenate([benchmark for _ in range(bunch)])\n",
    "    y_pred = []\n",
    "    for i in range(0, len(data_) - bunch, bunch):\n",
    "        digit1 = np.concatenate([duplicate_input(data_[j]) for j in range(i, i + bunch)])\n",
    "    \n",
    "        prediction_prob = tf.nn.softmax(logits)\n",
    "        feed_dict = {x1: digit1, x2: benchmark_, keep_prob: 1.0}\n",
    "        prediction_prob = sess.run(prediction_prob, feed_dict=feed_dict)\n",
    "        y_pred.extend(prediction_bunch(prediction_prob, bunch))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digit_mapping  = {i: j for (i, j) in zip(data.digits, range(10))}\n",
    "benchmark = create_benchmark(one_example_per_class)\n",
    "\n",
    "bunch = 128 # number of different images to test at the same time (batch size = 10 * bunch)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir=checkpoint_dir)\n",
    "    saver.restore(sess, latest_checkpoint)\n",
    "    print('%s was restored.' % latest_checkpoint)\n",
    "    for i, j in digit_mapping.iteritems():\n",
    "        print i, j\n",
    "        y_pred = test_number(getattr(data, i + '_test'), benchmark, sess, bunch=bunch)\n",
    "        y_true = [j] * len(y_pred)\n",
    "        print 'Accuracy for %i is %.3f' % (j, accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_results(digit1_, pred):\n",
    "    fig = plt.figure(figsize=(7,2))\n",
    "    b = fig.add_subplot(2, 1, 1)\n",
    "    b.axis('off')\n",
    "    b.imshow(digit1_[0], cmap='Greys_r')\n",
    "    b = fig.add_subplot(2, 1, 2)\n",
    "    b.axis('off')\n",
    "    b.imshow(benchmark[pred], cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_results(duplicate_input(data.sevens_test[221]), 7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
