{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One shot learning (UNFINISHED).\n",
    "\n",
    "Imagine we have a bunch of classes, say 10 classes, and we'd like to perform a classification task. However, we only have a good amount of data for 8 classes out of 10. For the other 2 classes, there's only a very limited number of examples (say 1 or 2 examples for instance). The idea of *one shot learning* is to train a network on the classes for which we have a lot of data and use this trained network to classify examples from the classes for which it wasn't trained on. Here, we mostly follow the approach described in *Siamese Neural Networks for One-shot Image Recognition* by Koch et al.\n",
    "\n",
    "We use a siamese architecture that we train on the MNIST data set. More specifically, we only train the netowk on digits from 0 to 7. The network will take two images and answer the following question: **do the two inputs belong to the same class?** After the training has been completed, we try to classify the digits 8 and 9 by comparing the testing examples to the very limited labeled data we have for these classes.\n",
    "\n",
    "For more details on *siamese architecture*, we refer the interested reader to the implementation of a siamese network in the notebook **siamese**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm # just for esthetics (progression bar)\n",
    "sys.path.insert(0, '../data_processing/')\n",
    "from siamese_data import MNIST # load the data and process it\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = MNIST()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning a similarity metric with a siamese network\n",
    "\n",
    "We are going to implement a siamese architecture similar to the one described in the **siamese notebook**, but with a stacked bi-directional LSTM network instead of a vanilla bi-directional LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first fix the hyperparameters for:\n",
    "- The training:\n",
    "    - Number of iterations,\n",
    "    - Learning rate,\n",
    "    - Batch size.\n",
    "- The network architectures:\n",
    "    - Number of stacked LSTMs,\n",
    "    - Number of neurons of each LSTM cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_iter = 15000 # maximum number of iterations for training\n",
    "learning_rate = 0.001\n",
    "batch_train = 128 # batch size for training\n",
    "batch_test = 512 # batch size for testing\n",
    "display = 100 # display the training loss and accuracy every `display` step\n",
    "n_test = 500 # how frequently to test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_inputs = 28 # dimension of each of the input vectors\n",
    "n_steps = 28 # sequence length\n",
    "n_hidden = [128, 64, 64] # number of neurons of each of the LSTM cell.\n",
    "n_classes = 2 # two possible classes, either `same` of `different`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    x1 = tf.placeholder(tf.float32, shape=[None, n_steps, n_inputs]) # placeholder for the first network (image 1)\n",
    "    x2 = tf.placeholder(tf.float32, shape=[None, n_steps, n_inputs]) # placeholder for the second network (image 2)\n",
    "\n",
    "    # placeholder for the label. `1` for `same` and `0` for `different`.\n",
    "    y = tf.placeholder(tf.int64, shape=[None])\n",
    "\n",
    "    # placeholder for dropout (we could use different dropout for different part of the architecture)\n",
    "    keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reshape_input(x_):\n",
    "    \"\"\"\n",
    "    Reshape the inputs to match the shape requirements of the function\n",
    "    `tf.nn.bidirectional_rnn`\n",
    "    \n",
    "    Args:\n",
    "        x_: a tensor of shape `(batch_size, n_steps, n_inputs)`\n",
    "        \n",
    "    Returns: a `list` of length `n_steps` with its elements being tensors\n",
    "    of shape `(batch_size, n_inputs)`\n",
    "    \"\"\"\n",
    "    x_ = tf.transpose(x_, [1, 0, 2]) # shape: (n_steps, batch_size, n_inputs)\n",
    "    x_ = tf.split(0, n_steps, x_) # a list of `n_steps` tensors of shape (1, batch_size, n_steps)\n",
    "    return [tf.squeeze(z, [0]) for z in x_] # remove size 1 dimension --> (batch_size, n_steps)\n",
    "\n",
    "\n",
    "x1_, x2_ = reshape_input(x1), reshape_input(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net(x_):\n",
    "    \"\"\"\n",
    "    Defines the network.\n",
    "    \n",
    "    Args:\n",
    "        x_: a tensor of shape `(batch_size, n_steps, n_inputs)` containing a batch\n",
    "            of images that will be fed to one of the two networks.\n",
    "    \n",
    "    Returns the last states from the forward and backward cell.\n",
    "    \"\"\"    \n",
    "    lstm_cells_fw = []\n",
    "    lstm_cells_bw = []\n",
    "    for hid_units in n_hidden:\n",
    "        lstm_cells_fw.append(tf.nn.rnn_cell.BasicLSTMCell(hid_units, forget_bias=1.0, state_is_tuple=True))\n",
    "        lstm_cells_bw.append(tf.nn.rnn_cell.BasicLSTMCell(hid_units, forget_bias=1.0, state_is_tuple=True))\n",
    "    stacked_lstm_fw = tf.nn.rnn_cell.MultiRNNCell(lstm_cells_fw, state_is_tuple=True)\n",
    "    stacked_lstm_bw = tf.nn.rnn_cell.MultiRNNCell(lstm_cells_bw, state_is_tuple=True)\n",
    "    \n",
    "    stacked_lstm_fw = tf.nn.rnn_cell.DropoutWrapper(stacked_lstm_fw, output_keep_prob=keep_prob)\n",
    "    stacked_lstm_bw = tf.nn.rnn_cell.DropoutWrapper(stacked_lstm_bw, output_keep_prob=keep_prob)\n",
    "    \n",
    "    \n",
    "    _, last_state_fw, last_state_bw = tf.nn.bidirectional_rnn(\n",
    "                                        stacked_lstm_fw, stacked_lstm_bw, x_,\n",
    "                                        dtype=tf.float32)\n",
    "    return last_state_fw, last_state_bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    with tf.variable_scope('siamese_network') as scope:\n",
    "        with tf.name_scope('network_1'):\n",
    "            last_state_fw1, last_state_bw1 = net(x1_)\n",
    "        with tf.name_scope('network_2'):\n",
    "            scope.reuse_variables() # tied weights (reuse the weights from `network_1` for `network_2`)\n",
    "            last_state_fw2, last_state_bw2 = net(x2_)\n",
    "\n",
    "    last_state1 = []\n",
    "    last_state2 = []\n",
    "    for i in range(len(n_hidden)):\n",
    "        for j in range(2):\n",
    "            last_state1.extend([last_state_bw1[i][j], last_state_fw1[i][j]])\n",
    "            last_state2.extend([last_state_bw2[i][j], last_state_fw2[i][j]])\n",
    "\n",
    "    last_state1 = tf.concat(1, last_state1) # We concatenate the states of the first network\n",
    "    last_state2 = tf.concat(1, last_state2) # We concatenate the states of the second network\n",
    "\n",
    "    # Weights and biases for the layer that connects the outputs from the two networks\n",
    "    weights = tf.get_variable('weigths_out', shape=[4 * np.sum(n_hidden), n_classes],\n",
    "                    initializer=tf.random_normal_initializer(stddev=1.0/float(np.sum(n_hidden))))\n",
    "    biases = tf.get_variable('biases_out', shape=[n_classes])\n",
    "\n",
    "    # difference between the states from the two networks\n",
    "    last_states_diff = tf.abs(last_state1 - last_state2) \n",
    "    logits = tf.matmul(last_states_diff, weights) + biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), y) \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_snapshot = 1000 # save the weights every `n_snapshot` step\n",
    "checkpoint_dir = '../models/one_shot_learning/'\n",
    "saver = tf.train.Saver() # to save the trained model and, later, to restore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network training begins.\n",
      "step 100, training loss: 0.29536, training accuracy: 0.883, 366.964 datum/sec\n",
      "step 200, training loss: 0.13712, training accuracy: 0.953, 367.446 datum/sec\n",
      "step 300, training loss: 0.08335, training accuracy: 0.969, 379.846 datum/sec\n",
      "step 400, training loss: 0.16318, training accuracy: 0.938, 383.843 datum/sec\n",
      "step 500, training loss: 0.06828, training accuracy: 0.969, 378.460 datum/sec\n",
      "testing step 500, accuracy 0.977\n",
      "step 600, training loss: 0.11439, training accuracy: 0.969, 392.944 datum/sec\n",
      "step 700, training loss: 0.11513, training accuracy: 0.977, 370.027 datum/sec\n",
      "step 800, training loss: 0.06252, training accuracy: 0.977, 374.315 datum/sec\n",
      "step 900, training loss: 0.05473, training accuracy: 0.984, 373.963 datum/sec\n",
      "step 1000, training loss: 0.04954, training accuracy: 0.984, 406.520 datum/sec\n",
      "testing step 1000, accuracy 0.988\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "../models/one_shot_learning/snapshot_1000.ckpt.tempstate1580720002495667802\n\t [[Node: save/save = SaveSlices[T=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/save/tensor_names, save/save/shapes_and_slices, beta1_power/_1033, beta2_power/_1035, biases_out/_1037, biases_out/Adam/_1039, biases_out/Adam_1/_1041, siamese_network/BiRNN_BW/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Bias/_1043, siamese_network/BiRNN_BW/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Bias/Adam/_1045, siamese_network/BiRNN_BW/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Bias/Adam_1/_1047, siamese_network/BiRNN_BW/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Matrix/_1049, siamese_network/BiRNN_BW/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Matrix/Adam/_1051, siamese_network/BiRNN_BW/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Matrix/Adam_1/_1053, siamese_network/BiRNN_BW/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Bias/_1055, siamese_network/BiRNN_BW/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Bias/Adam/_1057, siamese_network/BiRNN_BW/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Bias/Adam_1/_1059, siamese_network/BiRNN_BW/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Matrix/_1061, siamese_network/BiRNN_BW/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Matrix/Adam/_1063, siamese_network/BiRNN_BW/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Matrix/Adam_1/_1065, siamese_network/BiRNN_BW/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Bias/_1067, siamese_network/BiRNN_BW/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Bias/Adam/_1069, siamese_network/BiRNN_BW/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Bias/Adam_1/_1071, siamese_network/BiRNN_BW/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Matrix/_1073, siamese_network/BiRNN_BW/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Matrix/Adam/_1075, siamese_network/BiRNN_BW/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Matrix/Adam_1/_1077, siamese_network/BiRNN_FW/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Bias/_1079, siamese_network/BiRNN_FW/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Bias/Adam/_1081, siamese_network/BiRNN_FW/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Bias/Adam_1/_1083, siamese_network/BiRNN_FW/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Matrix/_1085, siamese_network/BiRNN_FW/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Matrix/Adam/_1087, siamese_network/BiRNN_FW/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Matrix/Adam_1/_1089, siamese_network/BiRNN_FW/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Bias/_1091, siamese_network/BiRNN_FW/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Bias/Adam/_1093, siamese_network/BiRNN_FW/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Bias/Adam_1/_1095, siamese_network/BiRNN_FW/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Matrix/_1097, siamese_network/BiRNN_FW/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Matrix/Adam/_1099, siamese_network/BiRNN_FW/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Matrix/Adam_1/_1101, siamese_network/BiRNN_FW/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Bias/_1103, siamese_network/BiRNN_FW/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Bias/Adam/_1105, siamese_network/BiRNN_FW/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Bias/Adam_1/_1107, siamese_network/BiRNN_FW/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Matrix/_1109, siamese_network/BiRNN_FW/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Matrix/Adam/_1111, siamese_network/BiRNN_FW/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Matrix/Adam_1/_1113, weigths_out/_1115, weigths_out/Adam/_1117, weigths_out/Adam_1/_1119)]]\nCaused by op u'save/save', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-a39fa9330f6e>\", line 3, in <module>\n    saver = tf.train.Saver() # to save the trained model and, later, to restore it.\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 845, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 513, in build\n    save_tensor = self._AddSaveOps(filename_tensor, vars_to_save)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 212, in _AddSaveOps\n    save = self.save_op(filename_tensor, vars_to_save)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 164, in save_op\n    tensor_slices=[vs.slice_spec for vs in vars_to_save])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/io_ops.py\", line 178, in _save\n    tensors, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 437, in _save_slices\n    data=data, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 711, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2260, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1230, in __init__\n    self._traceback = _extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-f08897cbbf69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# We save a snapshot of the weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mn_snapshot\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0msave_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'snapshot_'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.ckpt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Snapshot saved in file: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph)\u001b[0m\n\u001b[0;32m   1049\u001b[0m     model_checkpoint_path = sess.run(\n\u001b[0;32m   1050\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m         {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[0;32m   1052\u001b[0m     \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m     self._MaybeDeleteOldCheckpoints(model_checkpoint_path,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 372\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    373\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 636\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    637\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m       \u001b[1;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    706\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 708\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    709\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: ../models/one_shot_learning/snapshot_1000.ckpt.tempstate1580720002495667802\n\t [[Node: save/save = SaveSlices[T=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/save/tensor_names, save/save/shapes_and_slices, beta1_power/_1033, beta2_power/_1035, biases_out/_1037, biases_out/Adam/_1039, biases_out/Adam_1/_1041, siamese_network/BiRNN_BW/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Bias/_1043, siamese_network/BiRNN_BW/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Bias/Adam/_1045, siamese_network/BiRNN_BW/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Bias/Adam_1/_1047, siamese_network/BiRNN_BW/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Matrix/_1049, siamese_network/BiRNN_BW/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Matrix/Adam/_1051, siamese_network/BiRNN_BW/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Matrix/Adam_1/_1053, siamese_network/BiRNN_BW/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Bias/_1055, siamese_network/BiRNN_BW/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Bias/Adam/_1057, siamese_network/BiRNN_BW/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Bias/Adam_1/_1059, siamese_network/BiRNN_BW/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Matrix/_1061, siamese_network/BiRNN_BW/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Matrix/Adam/_1063, siamese_network/BiRNN_BW/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Matrix/Adam_1/_1065, siamese_network/BiRNN_BW/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Bias/_1067, siamese_network/BiRNN_BW/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Bias/Adam/_1069, siamese_network/BiRNN_BW/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Bias/Adam_1/_1071, siamese_network/BiRNN_BW/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Matrix/_1073, siamese_network/BiRNN_BW/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Matrix/Adam/_1075, siamese_network/BiRNN_BW/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Matrix/Adam_1/_1077, siamese_network/BiRNN_FW/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Bias/_1079, siamese_network/BiRNN_FW/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Bias/Adam/_1081, siamese_network/BiRNN_FW/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Bias/Adam_1/_1083, siamese_network/BiRNN_FW/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Matrix/_1085, siamese_network/BiRNN_FW/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Matrix/Adam/_1087, siamese_network/BiRNN_FW/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Matrix/Adam_1/_1089, siamese_network/BiRNN_FW/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Bias/_1091, siamese_network/BiRNN_FW/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Bias/Adam/_1093, siamese_network/BiRNN_FW/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Bias/Adam_1/_1095, siamese_network/BiRNN_FW/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Matrix/_1097, siamese_network/BiRNN_FW/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Matrix/Adam/_1099, siamese_network/BiRNN_FW/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Matrix/Adam_1/_1101, siamese_network/BiRNN_FW/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Bias/_1103, siamese_network/BiRNN_FW/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Bias/Adam/_1105, siamese_network/BiRNN_FW/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Bias/Adam_1/_1107, siamese_network/BiRNN_FW/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Matrix/_1109, siamese_network/BiRNN_FW/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Matrix/Adam/_1111, siamese_network/BiRNN_FW/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Matrix/Adam_1/_1113, weigths_out/_1115, weigths_out/Adam/_1117, weigths_out/Adam_1/_1119)]]\nCaused by op u'save/save', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-a39fa9330f6e>\", line 3, in <module>\n    saver = tf.train.Saver() # to save the trained model and, later, to restore it.\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 845, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 513, in build\n    save_tensor = self._AddSaveOps(filename_tensor, vars_to_save)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 212, in _AddSaveOps\n    save = self.save_op(filename_tensor, vars_to_save)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 164, in save_op\n    tensor_slices=[vs.slice_spec for vs in vars_to_save])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/io_ops.py\", line 178, in _save\n    tensors, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 437, in _save_slices\n    data=data, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 711, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2260, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1230, in __init__\n    self._traceback = _extract_stack()\n"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# the argument `allow_soft_placement=True` indicates that if a given function is\n",
    "# not implemented for GPUs, tensorflow will automatically use its CPU counterpart.\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "    sess.run(init) # initialize all variables\n",
    "    print('Network training begins.')\n",
    "    for i in range(1, max_iter + 1):\n",
    "        start = time.time()\n",
    "        # We retrieve a batch of data from the training set\n",
    "        batch_x1, batch_x2, batch_y = data.get_next_batch(batch_train, phase='train', one_shot=True)\n",
    "        # We feed the data to the network for training\n",
    "        feed_dict = {x1: batch_x1, x2: batch_x2, y: batch_y, keep_prob: .75}\n",
    "        _, loss_, accuracy_ = sess.run([optimizer, loss, accuracy], feed_dict=feed_dict)\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        if i % display == 0:\n",
    "            print('step %i, training loss: %.5f, training accuracy: %.3f, %.3f datum/sec' % (\n",
    "                    i, loss_, accuracy_, batch_train / elapsed))\n",
    "        \n",
    "        # Testing the network\n",
    "        if i % n_test == 0:\n",
    "            # Retrieving data from the test set\n",
    "            batch_x1, batch_x2, batch_y = data.get_next_batch(batch_test, phase='test', one_shot=True)\n",
    "            feed_dict = {x1: batch_x1, x2: batch_x2, y: batch_y, keep_prob: 1.0}\n",
    "            accuracy_test = sess.run(accuracy, feed_dict=feed_dict)\n",
    "            print('testing step %i, accuracy %.3f' % (i, accuracy_test))\n",
    "            \n",
    "            \n",
    "        # We save a snapshot of the weights\n",
    "        if i % n_snapshot == 0:\n",
    "            save_path = saver.save(sess, os.path.join(checkpoint_dir,'snapshot_') + str(i) + '.ckpt')\n",
    "            print('Snapshot saved in file: %s' % save_path)\n",
    "            \n",
    "    print('********************************')\n",
    "    print('Training finished.')\n",
    "    \n",
    "    # testing the trained network on a large sample\n",
    "    batch_x1, batch_x2, batch_y = data.get_next_batch(10000, phase='test', one_shot=True)\n",
    "    feed_dict = {x1: batch_x1, x2: batch_x2, y: batch_y, keep_prob:1.0}\n",
    "    accuracy_test = sess.run(accuracy, feed_dict=feed_dict)\n",
    "    print('********************************')\n",
    "    print('Testing the network.')\n",
    "    print('Network accuracy %.3f' % (accuracy_test))\n",
    "    print('********************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One shot learning: using the pretrained similarity metric on new classes\n",
    "\n",
    "We now want to see how the network performs on images from unseen classes, i.e. eights and nines.\n",
    "\n",
    "Following the approach described by Koch et al., we chose 10 images ($i_0, i_1,...,i_9$), one per class. We then classify an image by comparing it pairwise with the images $i_0,...,i_9$.\n",
    "\n",
    "But first, let's chose 10 reference images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_example_per_class = []\n",
    "for digit in data.digits:\n",
    "    one_example_per_class.append(\n",
    "        getattr(data, digit + '_train')[\n",
    "            np.random.randint(len(getattr(data, digit + '_train')))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we display the 10 images we use ($i_0,...,i_9$) as a benchmark for pairwise comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,2))\n",
    "for i in range(1, 11):\n",
    "    a = fig.add_subplot(2, 5, i)\n",
    "    a.axis('off')\n",
    "    image = one_example_per_class[i - 1].reshape((28, 28)) # reshape the image from (784) to (28, 28).\n",
    "    a.imshow(image, cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve the model trained above, and we classify images of 8 and 9 by comparing them with the benchmark images $i_0,...,i_9$. We report the accuracy of the classifcation on the *seen* and *unseen* classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reshape_input(image):\n",
    "    \"\"\"\n",
    "    Reshapes an image from `(784)` to `(1, 28, 28)`.\n",
    "    \n",
    "    Args:\n",
    "        image: a `numpy array` of shape `(784)`.\n",
    "    \n",
    "    Returns  a `numpy array` of shape `(1, 28, 28)`.\n",
    "    \"\"\"\n",
    "    image = np.expand_dims(image.reshape((28,28)), axis=0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_benchmark(images):\n",
    "    \"\"\"\n",
    "    Concatenates the 10 images of the benchmark into one tensor.\n",
    "    \n",
    "    Args:\n",
    "        images: a `list` of ten `numpy array`s of shape (784).\n",
    "    \n",
    "    Returns a `numpy array` of shape `(10, 28, 28)`.   \n",
    "    \"\"\"\n",
    "    images = [reshape_input(x) for x in images]\n",
    "    return np.concatenate(images)\n",
    "        \n",
    "\n",
    "def duplicate_input(image):\n",
    "    \"\"\"\n",
    "    Duplicates the image ten times.\n",
    "    \n",
    "    Args:\n",
    "        image: a `numpy array` of shape (784).\n",
    "    \n",
    "    Returns a `numpy array` of shape (10, 28, 28).\n",
    "    \"\"\"\n",
    "    image = reshape_input(image)\n",
    "    image = [image for x in range(10)]\n",
    "    return np.concatenate(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prediction_bunch(predictions, bunch=32):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        predictions: a `numpy array` of shape `(10 * bunch, 2)`. The second\n",
    "            column contains the probability that the given inputs are similar.\n",
    "        bunch: an `integer` equal the to the batch size divided by 10.\n",
    "    \n",
    "    Returns a list of length `bunch` containing the predicted labels, i.e.\n",
    "    a list of integers between 0 and 9.\n",
    "    \"\"\"\n",
    "    predictions_ = []\n",
    "    for i in range(bunch):\n",
    "        predictions_.append(np.argmax(predictions[10 * i : 10 * (i + 1), 1]))\n",
    "    return predictions_\n",
    "\n",
    "def test_number(data_, benchmark, sess, bunch=32):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data_: a `list` of `numpy array`s containing images from a specific class\n",
    "            (e.g. only 5 or only 9). The images have a shape `(784)`.\n",
    "        benchmark: a `numpy array` of shape `(10, 28, 28)`.\n",
    "        sess: a tensorflow session.\n",
    "        bunch: an `integer` equal the to the batch size divided by 10. It represents\n",
    "            the number of different images being fed to the network at the same time.\n",
    "    \n",
    "    Returns a list of length `bunch` containing the label predictions.\n",
    "    \"\"\"\n",
    "    benchmark_ = np.concatenate([benchmark for _ in range(bunch)])\n",
    "    y_pred = []\n",
    "    for i in range(0, len(data_) - bunch, bunch):\n",
    "        digit1 = np.concatenate([duplicate_input(data_[j]) for j in range(i, i + bunch)])\n",
    "    \n",
    "        prediction_prob = tf.nn.softmax(logits)\n",
    "        feed_dict = {x1: digit1, x2: benchmark_, keep_prob: 1.0}\n",
    "        prediction_prob = sess.run(prediction_prob, feed_dict=feed_dict)\n",
    "        y_pred.extend(prediction_bunch(prediction_prob, bunch))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digit_mapping  = {i: j for (i, j) in zip(data.digits, range(10))}\n",
    "benchmark = create_benchmark(one_example_per_class)\n",
    "\n",
    "bunch = 128 # number of different images to test at the same time (batch size = 10 * bunch)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir=checkpoint_dir)\n",
    "    saver.restore(sess, latest_checkpoint)\n",
    "    print('%s was restored.' % latest_checkpoint)\n",
    "    for i, j in digit_mapping.iteritems():\n",
    "        print i, j\n",
    "        y_pred = test_number(getattr(data, i + '_test'), benchmark, sess, bunch=bunch)\n",
    "        y_true = [j] * len(y_pred)\n",
    "        print 'Accuracy for %i is %.3f' % (j, accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_results(digit1_, pred):\n",
    "    fig = plt.figure(figsize=(7,2))\n",
    "    b = fig.add_subplot(2, 1, 1)\n",
    "    b.axis('off')\n",
    "    b.imshow(digit1_[0], cmap='Greys_r')\n",
    "    b = fig.add_subplot(2, 1, 2)\n",
    "    b.axis('off')\n",
    "    b.imshow(benchmark[pred], cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_results(duplicate_input(data.sevens_test[221]), 7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
